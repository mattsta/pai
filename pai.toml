# Polyglot AI Configuration File
# Define all your provider endpoints here.

# Optional: Path to a custom TOML or YAML file for model pricing.
# This can be used to override default prices or provide pricing for local models.
# See docs/PRICING.md for details.
# custom-pricing-file = "custom_prices.yaml"

[[endpoints]]
name = "featherless"
base_url = "https://api.featherless.ai/v1"
api_key_env = "FEATHERLESS_API_KEY" # Environment variable for the API key
# The adapter to use for the /chat/completions endpoint
chat_adapter = "openai_chat"
# The adapter to use for the legacy /completions endpoint
completion_adapter = "legacy_completion"
# Timeout for requests to this endpoint, in seconds.
timeout = 180

[[endpoints]]
name = "openai"
base_url = "https://api.openai.com/v1"
api_key_env = "OPENAI_API_KEY"
chat_adapter = "openai_chat"
# Timeout for requests to this endpoint, in seconds.
timeout = 180
# OpenAI deprecated /completions, so we don't define an adapter for it.

[[endpoints]]
name = "together"
base_url = "https://api.together.xyz/v1"
api_key_env = "TOGETHER_AI_KEY"
chat_adapter = "openai_chat"
completion_adapter = "legacy_completion"

[[endpoints]]
name = "mistral"
base_url = "https://api.mistral.ai/v1"
api_key_env = "MISTRAL_API_KEY"
chat_adapter = "openai_chat"

[[endpoints]]
name = "anthropic"
base_url = "https://api.anthropic.com/v1"
api_key_env = "ANTHROPIC_API_KEY"
chat_adapter = "anthropic"
# By default, we don't assume a model as Anthropic has many good ones.
# The user can specify one with --model claude-3-haiku-20240307, etc.

[[endpoints]]
# An example for a local model server
name = "local-lm-studio"
base_url = "http://localhost:1234/v1"
api_key_env = "LM_STUDIO_API_KEY" # Often not needed, can be set to "local"
chat_adapter = "openai_chat"
completion_adapter = "legacy_completion"

[[endpoints]]
name = "ollama"
base_url = "http://localhost:11434/api"
api_key_env = "OLLAMA_API_KEY" # Not used by Ollama, but framework expects it. Can be set to "ollama".
chat_adapter = "ollama"

[tool_config]
# Directories to scan for custom tool modules.
# Paths can be absolute or relative to the project root.
# Example: directories = ["/abs/path/to/tools", "custom_tools"]
directories = ["custom_tools"]

# --- PROFILES ---
# Define preset configurations for quick use.
# Any CLI flag can be set in a profile.
# e.g., `pai --profile research_haiku`
[profiles.research_haiku]
endpoint = "anthropic"
model = "claude-3-haiku-20240307"
temperature = 0.2
system = "You are a helpful research assistant."


# --- Multi-Model Arena Configurations ---
# Defines a set of participants for automated model-vs-model conversations.
# Use '/arena <name>' to start a session. Arena configurations are defined as
# sub-tables of `arenas`. The name of the arena (e.g., "debate") is used
# in the command.
[arenas.debate]
# `initiator` is the `id` of the participant who speaks first.
initiator = "proposer"

  # Each participant is defined as a table within `participants`.
  # The key of the table (e.g., "proposer") is its unique ID.
  [arenas.debate.participants.proposer]
  # Display name for this participant in the UI.
  name = "Proposer"
  # The endpoint to use for this participant. Must exist in [[endpoints]].
  endpoint = "openai"
  # The model to use.
  model = "gpt-4o-mini"
  # The key for the system prompt in the `prompts/` directory (without extension).
  system_prompt_key = "tutor"

  [arenas.debate.participants.critic]
  name = "Critic"
  endpoint = "openai"
  model = "gpt-4o-mini"
  system_prompt_key = "code_refactor"

  # An optional third participant who observes and summarizes the conversation.
  [arenas.debate.judge]
  name = "Judge"
  endpoint = "openai"
  model = "gpt-4o"
  system_prompt_key = "judge"

[arenas.cross_talk]
# An example of an arena with participants on different endpoints.
# This requires API keys for both endpoints to be configured.
initiator = "historian"

  [arenas.cross_talk.participants.historian]
  name = "Historian"
  endpoint = "openai"
  model = "gpt-4o-mini"
  system_prompt_key = "tutor"

  [arenas.cross_talk.participants.futurist]
  name = "Futurist"
  endpoint = "together"
  # This model must be supported by the 'together' endpoint.
  model = "meta-llama/Llama-3-8b-chat-hf"
  system_prompt_key = "code_refactor"
